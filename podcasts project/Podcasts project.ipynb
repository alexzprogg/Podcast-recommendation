{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7254abb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd \n",
    "rss_feed_url = 'https://feeds.buzzsprout.com/1973304.rss'\n",
    "page = requests.get(rss_feed_url)\n",
    "soup = BeautifulSoup(page.content,'xml')\n",
    "podcast_items = soup.find_all('item')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c36b11",
   "metadata": {},
   "source": [
    "# Retrieve podcast information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96f89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://podcasts.google.com/search/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fc9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = 'soccer' #user can input any word they want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dec7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = base_url + user_input \n",
    "\n",
    "resp = requests.get(search_url)\n",
    "soup = BeautifulSoup(resp.text, 'lxml') #utilizes google podcast api to search for podcast results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e84b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://podcasts.google.com/search/soccer'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aa9360",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "div_list = soup.find('div', role = 'list')\n",
    "results = soup.find_all('a', {'role': 'listitem'}) #find the podcasts items inside of the soup content \n",
    "podcast_urls = []\n",
    "domain_google_podcast = 'https://podcasts.google.com/'\n",
    "for result in results: \n",
    "    podcast_url_part = result.get('href')[2:] #get the links of each podcast item \n",
    "#     podcast_url1 = domain_google_podcast+podcast_url_part\n",
    "#     print(str(podcast_url1))\n",
    "    podcast_urls.append(domain_google_podcast+podcast_url_part)\n",
    "print(podcast_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ebd75",
   "metadata": {},
   "source": [
    "## Get RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243bae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_podcast_url = 'https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5tZWdhcGhvbmUuZm0vVEFNQzE5NzkwMDE4NTk?sa=X&ved=0CAMQ4aUDahcKEwiI9OD5xrr9AhUAAAAAHQAAAAAQDg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1d3dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_getrssfeed = 'https://getrssfeed.com'\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "622fffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get podcast homepage url \n",
    "r = requests.post(url_getrssfeed, data={\"url\":google_podcast_url}, headers=headers) \n",
    "soup_getrssafterpost = BeautifulSoup(r.text, 'lxml')\n",
    "results = soup_getrssafterpost.find_all('script', type='text/javascript')\n",
    "\n",
    "for result in results:\n",
    "    if 'http' in result.text:\n",
    "        podcast_homepage_url_raw = result.text\n",
    "        print(\"Find!\")\n",
    "        break\n",
    "podcast_homepage_url = re.findall(r'\"(.*?)\"', podcast_homepage_url_raw)[0]\n",
    "print(\"podcast homepage url = \"+podcast_homepage_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cefea9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get podcast homepage url rss feed \n",
    "resp = requests.get(podcast_homepage_url)\n",
    "soup = BeautifulSoup(resp.text, 'lxml')\n",
    "a_list = soup.find_all('a',{\"target\":\"_blank\"})\n",
    "for a in a_list:\n",
    "    if a.text == \"\\n RSS feed\\n\":\n",
    "        rss_url = a[\"href\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "05abca89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://feeds.megaphone.fm/tamc1979001859'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe4f8f",
   "metadata": {},
   "source": [
    "## Parse RSS feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66bd88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57090a6e",
   "metadata": {},
   "source": [
    "## Create transcript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55871c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('.\\downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b02644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_title(title):\n",
    "    file_name = re.sub(r'[%/&!@#\\*\\$\\?\\+\\^\\\\.\\\\\\\\]', '', title)[:100]\n",
    "    return file_name\n",
    "#file_name = title.replace('/','-').replace('\\\\\\\\','-').replace('.',' ')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "    title = podcast.find('title').text\n",
    "    mp3_url = podcast.find('enclosure')['url']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce7f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "website_links = [\"https://www.sueddeutsche.de/\"]\n",
    "\n",
    "rss_feeds = []\n",
    "\n",
    "def check_for_real_rss(url):\n",
    "    base_url = re.search('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:\\/\\n]+)',url).group(0)\n",
    "    print(base_url)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    for e in soup.select('[type=\"application/rss+xml\"],a[href*=\".rss\"],a[href$=\"feed\"]'):\n",
    "        if e.get('href').startswith('/'):\n",
    "            rss = (base_url+e.get('href'))\n",
    "        else:\n",
    "            rss = (e.get('href'))\n",
    "        if 'xml' in requests.get(rss).headers.get('content-type'):\n",
    "            rss_feeds.append(rss)\n",
    "podcast = []\n",
    "for url in website_links:\n",
    "    soup = BeautifulSoup(requests.get(url).text)\n",
    "    for e in soup.select('a[href*=\"rss\"],a[href*=\"/feed\"],a:-soup-contains-own(\"RSS\")'):\n",
    "        if e.get('href').startswith('/'):\n",
    "            check_for_real_rss(url.strip('/')+e.get('href'))\n",
    "        else:\n",
    "            check_for_real_rss(e.get('href'))\n",
    "    print(rss_feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3db1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for podcast in podcast_items:\n",
    "    if count == 10:\n",
    "        break\n",
    "        \n",
    "    title = podcast.find('title').text\n",
    "    mp3_url = podcast.find('enclosure')['url']\n",
    "    print(title+\": \"+mp3_url)\n",
    "    mp3_file = requests.get(mp3_url)\n",
    "    title = simplify_title(title).replace('|','').replace(' ','_')\n",
    "    with open(f\"downloads//{title}.mp3\", 'wb') as f:\n",
    "        f.write(mp3_file.content)\n",
    "        print(title,count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b16561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, chunk_size=5242880):\n",
    "    with open(filename, 'rb') as _file:\n",
    "        while True:\n",
    "            data = _file.read(chunk_size)\n",
    "            if not data:\n",
    "                break\n",
    "            yield data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68361e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'authorization': '4bec9f0712d7478f8a04495dea6afd94'}\n",
    "\n",
    "#urls = []\n",
    "files = 'downloads/'\n",
    "file_names = os.listdir(files)\n",
    "count = 0\n",
    "mapping_table = []\n",
    "for file in file_names:\n",
    "    filename = f'{files}/{file}'\n",
    "    response = requests.post('https://api.assemblyai.com/v2/upload',\n",
    "                            headers=headers,\n",
    "                            data=read_file(filename))\n",
    "    \n",
    "    upload_response = response.json()\n",
    "    \n",
    "    print(\"Transcription #\", count)\n",
    "    endpoint = \"https://api.assemblyai.com/v2/transcript\"\n",
    "    json = {\n",
    "        \"audio_url\": upload_response['upload_url'],\n",
    "        \"audio_start_from\": 300000,\n",
    "        \"audio_end_at\": 600000,\n",
    "    }\n",
    "    headers = {\n",
    "        \"authorization\": '4bec9f0712d7478f8a04495dea6afd94',\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    transcription_response = requests.post(endpoint, json=json, headers=headers)\n",
    "    print(transcription_response)\n",
    "    transcription_id = transcription_response.json()['id']\n",
    "    count+=1\n",
    "    mapping_table.append([filename,upload_response['upload_url'],transcription_id])\n",
    "    if(count==3):\n",
    "        break\n",
    "print(mapping_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_table = pd.DataFrame(mapping_table, columns=['filename','upload_url','transcript_id'])\n",
    "transcription_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4caa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_table['filename'] = transcription_table['filename'].str.replace('downloads//','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in transcription_table.iterrows():\n",
    "    filename = row['filename']\n",
    "    transcription_id = row['transcript_id']\n",
    "    transcript_url = f'https://api.assemblyai.com/v2/transcript/{transcription_id}'\n",
    "\n",
    "    transcription_result = requests.get(transcript_url, headers=headers)\n",
    "    print(transcription_result.json()['text'])\n",
    "    raw_text = transcription_result.json()['text']\n",
    "    with open(f'./transcripts/{filename}.txt', 'w') as f:\n",
    "        f.write(raw_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
